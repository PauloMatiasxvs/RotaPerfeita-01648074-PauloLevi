// Classe que implementa o algoritmo Q-Learning
class QLearning {
    constructor(env) {
        this.env = env;
        this.qTable = this.initializeQTable();
        this.alpha = 0.1; // Taxa de aprendizado
        this.gamma = 0.9; // Fator de desconto
        this.epsilon = 1.0; // Taxa de explora√ß√£o inicial
        this.epsilonMin = 0.1; // Taxa m√≠nima
        this.epsilonDecay = 0.995; // Decaimento
        this.episode = 0; // Contador de epis√≥dios
        this.totalReward = 0; // Recompensa acumulada
        this.rewards = []; // Hist√≥rico de recompensas
        this.maxEpisodes = 1000; // Limite de epis√≥dios
        logDebug('üß† Q-Learning inicializado com Œ±=0.1, Œ≥=0.9, Œµ=1.0');
    }

    // Inicializa a Q-Table com zeros
    initializeQTable() {
        logDebug('üìä Inicializando Q-Table...');
        const states = this.env.gridSize * this.env.gridSize;
        const actions = this.env.actions.length;
        const qTable = new Array(states);
        for (let i = 0; i < states; i++) {
            qTable[i] = new Array(actions);
            for (let j = 0; j < actions; j++) {
                qTable[i][j] = 0;
            }
        }
        logDebug(`üìã Q-Table criada: ${states} estados, ${actions} a√ß√µes`);
        return qTable;
    }

    // Escolhe uma a√ß√£o usando pol√≠tica Œµ-greedy
    chooseAction(state) {
        logDebug(`ü§î Escolhendo a√ß√£o para o estado ${state}`);
        const randomValue = Math.random();
        if (randomValue < this.epsilon) {
            const action = this.env.actions[Math.floor(Math.random() * this.env.actions.length)];
            logDebug(`üå™Ô∏è Explora√ß√£o: A√ß√£o aleat√≥ria ${this.env.actionToString(action)} (Œµ=${this.epsilon.toFixed(2)})`);
            return action;
        } else {
            const qValues = this.qTable[state];
            let maxQ = qValues[0];
            let bestAction = 0;
            for (let i = 1; i < qValues.length; i++) {
                if (qValues[i] > maxQ) {
                    maxQ = qValues[i];
                    bestAction = i;
                }
            }
            logDebug(`üéØ Explora√ß√£o: Melhor a√ß√£o ${this.env.actionToString(bestAction)} com Q=${maxQ.toFixed(2)}`);
            return bestAction;
        }
    }

    // Atualiza a Q-Table com base na recompensa
    updateQTable(state, action, reward, nextState) {
        logDebug(`üîß Atualizando Q-Table: Estado ${state}, A√ß√£o ${this.env.actionToString(action)}`);
        const currentQ = this.qTable[state][action];
        const nextQValues = this.qTable[nextState];
        let maxNextQ = nextQValues[0];
        for (let i = 1; i < nextQValues.length; i++) {
            if (nextQValues[i] > maxNextQ) {
                maxNextQ = nextQValues[i];
            }
        }
        const newQ = currentQ + this.alpha * (reward + this.gamma * maxNextQ - currentQ);
        this.qTable[state][action] = newQ;
        logDebug(`üìà Novo valor Q: ${newQ.toFixed(2)} (Recompensa: ${reward}, Max Q pr√≥ximo: ${maxNextQ.toFixed(2)})`);
    }

    // Executa um epis√≥dio completo de treinamento
    trainEpisode() {
        logDebug(`üèÅ Iniciando epis√≥dio ${this.episode}`);
        let state = this.env.reset();
        let episodeReward = 0;
        let terminal = false;
        let stepCount = 0;

        while (!terminal) {
            logDebug(`üîÑ Passo ${stepCount} do epis√≥dio ${this.episode}`);
            const action = this.chooseAction(state);
            const [nextState, reward, isTerminal] = this.env.step(action);
            this.updateQTable(state, action, reward, nextState);
            state = nextState;
            episodeReward += reward;
            terminal = isTerminal;
            stepCount++;
            if (stepCount > this.env.maxSteps) {
                logDebug('‚ö†Ô∏è Limite de passos atingido, for√ßando t√©rmino do epis√≥dio');
                terminal = true;
            }
        }

        this.totalReward += episodeReward;
        this.rewards.push(episodeReward);
        this.epsilon = Math.max(this.epsilonMin, this.epsilon * this.epsilonDecay);
        this.episode++;
        logDebug(`‚úÖ Epis√≥dio ${this.episode} conclu√≠do. Recompensa: ${episodeReward}, Novo Œµ: ${this.epsilon.toFixed(2)}`);
    }

    // Verifica se o treinamento terminou
    isTrainingComplete() {
        const complete = this.episode >= this.maxEpisodes;
        if (complete) {
            logDebug('üèÜ Treinamento conclu√≠do: M√°ximo de epis√≥dios atingido');
        }
        return complete;
    }
}