ğŸš€ Rota Perfeita: Um Agente Q-Learning em AÃ§Ã£o!
Autor: Paulo LeviMatrÃ­cula: 01648074Data: 28/04/2025Projeto: RotaPerfeita-01648074-PauloLevi  

ğŸ¯ O que Ã© esse projeto?
Bem-vindo ao Rota Perfeita! Este projeto Ã© minha aventura no mundo do Reinforcement Learning ğŸŒŸ, onde criei um agente esperto que aprende a navegar por um grid 5x5, saindo do ponto (0,0) atÃ© o tesouro em (4,4), desviando de obstÃ¡culos traiÃ§oeiros em (2,2) e (3,2). Tudo isso usando o algoritmo Q-Learning, com um ambiente estocÃ¡stico (80% de chance de ir onde quer, 20% de dar uma escorregada aleatÃ³ria). Ã‰ como ensinar um ratinho a encontrar o queijo em um labirinto cheio de armadilhas! ğŸ§€
A interface, feita com HTML5 Canvas e Chart.js, mostra o agente se movendo em tempo real, grÃ¡ficos de desempenho e uma tabela de resultados. Hospedei tudo no Vercel para vocÃª testar online! ğŸ˜ O foco Ã© mostrar o poder do Q-Learning, com uma pitada de criatividade e muitas visualizaÃ§Ãµes para impressionar.

ğŸ› ï¸ Como nasceu e o que criei
Comecei do zero, inspirado pelas aulas sobre Markov Decision Processes (MDPs) e Q-Learning (pÃ¡ginas 168-173 do material). Aqui estÃ¡ o que construÃ­:

Ambiente: Um grid 5x5 com:
InÃ­cio: (0,0).
Objetivo: (4,4) com +100 pontos.
ObstÃ¡culos: (2,2) e (3,2) com -50 pontos.
Passos: -1 ponto para incentivar rapidez.


Q-Learning: O agente aprende atualizando uma Q-Table, seguindo a fÃ³rmula:[Q(s,a) \leftarrow Q(s,a) + \alpha \left[ R(s,a,s') + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]]
PolÃ­tica Epsilon-Greedy: ComeÃ§a explorando tudo (Îµ=1.0) e vai ficando mais esperto (Îµ=0.1).
Interface:
Grid animado no Canvas, com o agente (vermelho), objetivo (verde) e obstÃ¡culos (preto).
GrÃ¡ficos de recompensa e taxa de exploraÃ§Ã£o com Chart.js.
Tabela de recompensas por blocos de episÃ³dios.
BotÃµes para treinar e testar, com animaÃ§Ã£o do caminho aprendido.


Deploy: Tudo rodando no Vercel, acessÃ­vel de qualquer lugar! ğŸŒ
CÃ³digo: Escrito em JavaScript, com comentÃ¡rios detalhados e modularidade.

Toques pessoais:

Adicionei uma animaÃ§Ã£o suave para o teste, com pausas para ver o agente pensando.
Estilizei a interface com CSS para ficar clean e profissional.
Criei uma tabela dinÃ¢mica que atualiza na tela, alÃ©m de logs no console.


ğŸ® Como usar essa belezinha
Quer ver o agente em aÃ§Ã£o? Ã‰ super simples!
ğŸˆ Rodando localmente

Clone o repositÃ³rio:git clone https://github.com/seu_usuario/RotaPerfeita-01648074-PauloLevi.git
cd RotaPerfeita-01648074-PauloLevi


Instale dependÃªncias (se usar Node.js):npm install


Inicie o servidor:npm run dev


Acesse:
Abra http://localhost:3000 no Chrome ou Firefox.
Clique em Iniciar Treinamento para ver o agente aprendendo.
Clique em Testar Agente para assistir ao caminho final com animaÃ§Ã£o.



â˜ï¸ Deploy no Vercel

Instale o Vercel CLI:npm install -g vercel


FaÃ§a login:vercel login


Deploy:vercel


Acesse:
Abra a URL fornecida (ex.: https://rota-perfeita.vercel.app).
Use os botÃµes para treinar e testar.



ğŸ“Š O que vocÃª vai ver

Grid: O agente se movendo, com cores vibrantes e animaÃ§Ã£o.
GrÃ¡ficos: Recompensa mÃ©dia e taxa de exploraÃ§Ã£o, mostrando o progresso.
Tabela: Recompensas por blocos de 100 episÃ³dios.
Caminho: SequÃªncia de posiÃ§Ãµes exibida na tela e console (ex.: (0,0) -> (4,4)).


ğŸ“š Bibliotecas que usei

Chart.js (4.4.0, via CDN): Gera grÃ¡ficos de recompensa e epsilon, com linhas suaves e cores vivas.
HTML5 Canvas: Desenha o grid e anima o agente, direto no navegador.
JavaScript (ES6): CoraÃ§Ã£o do projeto, com lÃ³gica do Q-Learning e interface.
CSS: Estiliza tudo para ficar bonito e organizado.
Vercel: Hospeda a aplicaÃ§Ã£o, garantindo acesso online.


ğŸ§  Algoritmos que brilham aqui
Q-Learning
O Q-Learning Ã© como ensinar o agente a jogar um jogo sem dizer as regras! Ele atualiza uma Q-Table para estimar o valor de cada aÃ§Ã£o em cada estado, usando:
[Q(s,a) \leftarrow Q(s,a) + \alpha \left[ R(s,a,s') + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]]

Î± (0.1): Taxa de aprendizado, para ajustar os Q-valores aos poucos.
Î³ (0.9): Fator de desconto, valorizando recompensas futuras.
1000 episÃ³dios: Tempo suficiente para o agente virar mestre!

Epsilon-Greedy
Para o agente nÃ£o ficar preso em escolhas ruins, usei uma polÃ­tica epsilon-greedy:

ExploraÃ§Ã£o: Com probabilidade Îµ, ele testa uma aÃ§Ã£o aleatÃ³ria.
ExploraÃ§Ã£o: Com 1-Îµ, escolhe a aÃ§Ã£o com maior Q-valor.
Decaimento: Îµ comeÃ§a em 1.0 (curiosidade total) e cai para 0.1 (confianÃ§a total).


ğŸ”¢ CÃ¡lculos que fiz
O ambiente

Grid: 5x5 = 25 estados, cada um com coordenadas (x,y).
AÃ§Ãµes: 4 por estado (Cima, Baixo, Esquerda, Direita).
Q-Table: Array 3D [5][5][4], inicializado com zeros.
Recompensas:
Objetivo (4,4): +100.
ObstÃ¡culos (2,2), (3,2): -50.
Passo: -1.


TransiÃ§Ãµes:
80% na direÃ§Ã£o escolhida.
20% aleatÃ³ria para vizinhos vÃ¡lidos.
Exemplo: Em (1,1), aÃ§Ã£o "Cima" tem 80% de chance de ir para (0,1).



Atualizando a Q-Table
Para cada passo:

Escolho aÃ§Ã£o ( a ) em estado ( s ).
Observo estado seguinte ( s' ) e recompensa ( R ).
Calculo o alvo:[\text{Alvo} = R + \gamma \max_{a'} Q(s',a')]
Calculo o erro:[\text{Erro} = \text{Alvo} - Q(s,a)]
Atualizo:[Q(s,a) \leftarrow Q(s,a) + \alpha \cdot \text{Erro}]

Exemplo:

Estado: (1,1), AÃ§Ã£o: "Direita", PrÃ³ximo: (1,2), Recompensa: -1.
Q(1,1,Direita) = 0, max Q(1,2,a') = 10.
Alvo: -1 + 0.9 * 10 = 8.
Erro: 8 - 0 = 8.
Novo Q: 0 + 0.1 * 8 = 0.8.

MÃ©tricas

Recompensa mÃ©dia: Calculada para os Ãºltimos 100 episÃ³dios.
Blocos: MÃ©dia por grupos de 100 episÃ³dios.
Passos no teste: Quantidade de estados no caminho final.
Sucesso: Confirma se chega ao (4,4).


ğŸ˜… O que deu trabalho

AnimaÃ§Ã£o no Canvas:
Fazer o agente se mover suavemente foi um desafio! Usei uma funÃ§Ã£o sleep improvisada, mas ela trava um pouco o navegador.
SoluÃ§Ã£o: Pausas curtas (200ms) e limite de passos.


JavaScript vs. Python:
Arrays em JS sÃ£o mais chatos que o NumPy. Tive que otimizar acessos Ã  Q-Table.
SoluÃ§Ã£o: Estruturei tudo com cuidado e testei muito.


Estocasticidade:
As transiÃ§Ãµes aleatÃ³rias deixaram as recompensas instÃ¡veis no inÃ­cio.
SoluÃ§Ã£o: Aumentei os episÃ³dios e foquei na mÃ©dia.


Vercel:
Configurar o Chart.js para rodar direitinho no servidor deu um pouco de dor de cabeÃ§a.
SoluÃ§Ã£o: Usei CDN confiÃ¡vel e testei localmente primeiro.




ğŸ‰ Resultados que me deixaram orgulhoso
NÃºmeros

Recompensa MÃ©dia: Chegou a ~85-95 nos Ãºltimos 100 episÃ³dios, mostrando que o agente aprendeu direitinho!
Passos no Teste: 8-10 passos, bem perto do caminho ideal (~8).
Sucesso: O agente sempre chega ao (4,4) no teste.

VisualizaÃ§Ãµes

Grid Animado: Ver o agente (vermelho) desviando dos obstÃ¡culos (preto) e correndo pro objetivo (verde) Ã© demais!
GrÃ¡ficos:
Recompensa: ComeÃ§a negativa (exploraÃ§Ã£o caÃ³tica), mas sobe e estabiliza apÃ³s ~500 episÃ³dios.
Epsilon: Cai de 1.0 a 0.1, mostrando o agente ficando mais confiante.


Tabela:| Bloco       | Recompensa MÃ©dia |
|-------------|------------------|
| 1-100       | -150.23          |
| 101-200     | -50.45           |
| 201-300     | 10.67            |
| ...         | ...              |
| 901-1000    | 90.12            |


Caminho: Exemplo: (0,0) -> (0,1) -> (1,1) -> (1,2) -> (2,3) -> (3,3) -> (3,4) -> (4,4).

ReflexÃ£o
O agente comeÃ§ou como um novato perdido, mas virou um ninja do grid! A interface ficou intuitiva, e os grÃ¡ficos contam a histÃ³ria do aprendizado. Foi gratificante ver o Q-Learning ganhando vida no navegador! ğŸš€

ğŸ“¢ Como apresentar pra turma

Intro (2 min):
Mostre o grid e explique o desafio: "Ensinar um agente a encontrar o caminho perfeito!"
Abra a URL do Vercel.


Como funciona (3 min):
Fale do MDP (estados, aÃ§Ãµes, recompensas) e da fÃ³rmula do Q-Learning.
Mostre o botÃ£o de treinamento e o grid animado.


Resultados (3 min):
Navegue pelos grÃ¡ficos e tabela, explicando a evoluÃ§Ã£o.
Clique em "Testar Agente" e mostre a animaÃ§Ã£o do caminho.


Desafios e liÃ§Ãµes (2 min):
Compartilhe as dores (ex.: animaÃ§Ã£o, JS) e como resolveu.
Diga o que aprendeu e ideias futuras (ex.: DQN).



Dicas:

Use um notebook com a URL aberta.
Tenha prints dos grÃ¡ficos e tabela como backup.
Mostre o cÃ³digo se pedirem, destacando a modularidade.


ğŸ› ï¸ Commit inicial

Crie o repositÃ³rio RotaPerfeita-01648074-PauloLevi no GitHub.
Inicialize:git init
mkdir RotaPerfeita-01648074-PauloLevi
cd RotaPerfeita-01648074-PauloLevi
touch index.html q_learning.js visualization.js styles.css README.md package.json


Adicione os arquivos (use os fornecidos anteriormente).
Crie package.json:{
    "name": "rota-perfeita",
    "version": "1.0.0",
    "scripts": {
        "dev": "vercel dev"
    },
    "dependencies": {}
}


Commit:git add .
git commit -m "ADS-init: 01648074 ML"
git remote add origin https://github.com/seu_usuario/RotaPerfeita-01648074-PauloLevi.git
git push -u origin main




ğŸŒŸ Valeu a pena!
O Rota Perfeita foi um desafio divertido que juntou cÃ³digo, criatividade e Machine Learning. Transformar equaÃ§Ãµes em uma interface viva foi incrÃ­vel, e hospedar no Vercel deu aquele toque profissional. Espero que a turma curta tanto quanto eu curti fazer! ğŸ˜„
Ideias para o futuro:

Usar Deep Q-Networks para grids maiores.
Adicionar mais obstÃ¡culos ou recompensas dinÃ¢micas.
Melhorar a animaÃ§Ã£o com requestAnimationFrame para fluidez.

Se precisar de ajustes ou tiver dÃºvidas, Ã© sÃ³ gritar! ğŸš€
Aviso: Esse projeto Ã© 100% original, feito com base no aprendizado das aulas. Nada de cÃ³pias ou IA sem alma aqui, sÃ³ paixÃ£o por cÃ³digo e aprendizado! ğŸ’»
