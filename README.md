ğŸ¤– RobÃ´ Coletor de Recursos com Q-Learning ğŸš€
ğŸ¯ IntroduÃ§Ã£o ao Projeto
Oi, galera! Esse projeto Ã© uma baita demonstraÃ§Ã£o de Aprendizado por ReforÃ§o com Q-Learning! Nele, um robÃ´ virtual aprende a coletar moedas ğŸ’° em um grid 10x10, desviando de paredes ğŸ§±. O ambiente Ã© um Processo de DecisÃ£o de Markov (MDP), com estados, aÃ§Ãµes (cima, baixo, esquerda, direita) e recompensas bem definidas. Fiz tudo em JavaScript, HTML e CSS, sem bibliotecas externas, pra rodar direitinho no navegador e ser implantado no Vercel. ğŸŒ
O projeto cumpre todos os requisitos:

ExploraÃ§Ã£o vs. ExploraÃ§Ã£o: PolÃ­tica Îµ-greedy com decaimento.
AtualizaÃ§Ã£o DinÃ¢mica: Q-Table atualizada a cada passo.
Recompensas/Penalidades: +100 por moeda, -10 por parede, e mais.
VisualizaÃ§Ã£o: Grade de divs e tabela de recompensas.

ğŸ› ï¸ Tecnologias e Bibliotecas

HTML5: Estrutura com grade de divs e tabela.
CSS3: Estilo vibrante com animaÃ§Ãµes.
JavaScript (puro): Toda a lÃ³gica, do Q-Learning Ã  renderizaÃ§Ã£o.
Nenhuma biblioteca externa! Tudo feito na raÃ§a! ğŸ’ª

ğŸ§  Algoritmos Aplicados
Q-Learning
O coraÃ§Ã£o do projeto Ã© o Q-Learning, que atualiza a Q-Table com:[ Q(s, a) \leftarrow Q(s, a) + \alpha \cdot (r + \gamma \cdot \max Q(s', a') - Q(s, a)) ]

( \alpha = 0.1 ): Taxa de aprendizado, pra ajustar o quanto o robÃ´ "absorve" cada experiÃªncia.
( \gamma = 0.9 ): Fator de desconto, pra valorizar recompensas futuras.
( s ): Estado (posiÃ§Ã£o do robÃ´).
( a ): AÃ§Ã£o (cima, baixo, esquerda, direita).
( r ): Recompensa.
( s' ): PrÃ³ximo estado.

PolÃ­tica Îµ-Greedy
Pra balancear exploraÃ§Ã£o (tentar aÃ§Ãµes novas) e exploraÃ§Ã£o (usar o que jÃ¡ sabe):

Com probabilidade ( \epsilon ), escolhe uma aÃ§Ã£o aleatÃ³ria.
Com probabilidade ( 1 - \epsilon ), escolhe a aÃ§Ã£o com maior Q.
( \epsilon ) comeÃ§a em 1.0 (100% aleatÃ³rio) e decai pra 0.1 (quase sempre a melhor aÃ§Ã£o).

ğŸ“ CÃ¡lculos e FÃ³rmulas

Q-Table: Uma matriz gigante de 100 estados (10x10) x 4 aÃ§Ãµes, toda zerada no inÃ­cio:[ Q(s, a) = 0 \text{ para } s \in {0, ..., 99}, a \in {0, 1, 2, 3} ]
Estado: Calculado como:[ s = \text{linha} \cdot 10 + \text{coluna} ]Exemplo: PosiÃ§Ã£o (5,5) â†’ ( s = 5 \cdot 10 + 5 = 55 ).
Recompensas:
Coletar moeda: +100 ğŸ’°
Bater em parede: -10 ğŸ§±
Cada movimento: -1 ğŸš¶
Tentar sair do grid: -5 ğŸš«


AtualizaÃ§Ã£o da Q-Table:[ Q(s, a) \leftarrow Q(s, a) + 0.1 \cdot (r + 0.9 \cdot \max Q(s', a') - Q(s, a)) ]
Decaimento de Îµ:[ \epsilon \leftarrow \max(0.1, \epsilon \cdot 0.995) ]

ğŸš€ Como Executar o Projeto

Clonar o RepositÃ³rio:
git clone <URL_DO_REPOSITORIO>
cd robo-coletor-qlearning


Rodar Localmente:

Com servidor (recomendado):npm install -g http-server
http-server

Acesse http://localhost:8080 no navegador.
Sem servidor:Abra index.html direto no navegador (pode ter restriÃ§Ãµes de CORS).


Interagir:

Clique em Iniciar Treinamento â–¶ï¸ pra ver o robÃ´ aprender.
Clique em Pausar Treinamento â¸ï¸ pra dar uma pausa.
Clique em Exibir MÃ©tricas ğŸ“Š pra ver o progresso.
Abra o console (F12) pra logs detalhados.


Implantar no Vercel:

Crie um repositÃ³rio Git:git init
git add .
git commit -m "Projeto RobÃ´ Coletor"
git remote add origin <URL>
git push origin main


No Vercel, importe o repositÃ³rio, configure como projeto estÃ¡tico, e implante.



ğŸ“Š Resultados e ComentÃ¡rios

O que rolou:

ApÃ³s 1000 episÃ³dios, o robÃ´ vira ninja e coleta moedas rapidinho, desviando das paredes.
As recompensas sobem na tabela HTML, mostrando que o aprendizado tÃ¡ funcionando.
O Îµ cai de 1.0 pra 0.1, provando que o robÃ´ tÃ¡ ficando esperto.


MÃ©tricas:

EpisÃ³dio Atual: AtÃ© 1000.
Recompensa Total: Soma de todas as recompensas.
Taxa de ExploraÃ§Ã£o (Îµ): Cai pra 0.1, indicando aÃ§Ãµes otimizadas.


VisualizaÃ§Ã£o:

Grade de Divs: Mostra o robÃ´ (azul), moedas (dourado) e paredes (vermelho) com animaÃ§Ãµes.
Tabela HTML: Lista as recompensas por episÃ³dio, atualizada ao vivo.
MÃ©tricas: EpisÃ³dio, recompensa total e Îµ na interface.


Dificuldades:

Balancear Recompensas: Tive que testar vÃ¡rias vezes pra achar valores que fizessem o robÃ´ aprender sem desanimar (tipo, -100 era muito pesado).
Atualizar a Grade: Garantir que os divs mudassem direitinho sem travar foi um desafio.
Debugging: Adicionei um monte de logs pra caÃ§ar erros, porque Ã s vezes o treinamento nÃ£o iniciava.


