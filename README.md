# üè∞ Explorador do Labirinto 2D com Q-Learning

## üìñ Introdu√ß√£o ao Projeto
Bem-vindo ao **Explorador do Labirinto 2D**! Este √© um projeto educativo de Machine Learning que utiliza o algoritmo **Q-Learning** para ensinar um rob√¥ virtual a navegar em um labirinto 5x5. O objetivo do rob√¥ √© encontrar um tesouro enquanto evita obst√°culos e otimiza seus movimentos. O rob√¥ aprende por tentativa e erro, ganhando recompensas ao alcan√ßar o tesouro e penalidades ao encontrar obst√°culos ou dar passos desnecess√°rios.

Este projeto √© ideal para quem quer entender os conceitos b√°sicos de aprendizado por refor√ßo de forma pr√°tica e visual. Ele demonstra como um agente pode aprender a tomar decis√µes inteligentes em um ambiente simples, aplicando t√©cnicas como explora√ß√£o vs. explora√ß√£o e atualiza√ß√£o din√¢mica de conhecimento.

---

## üõ†Ô∏è Tecnologias e Bibliotecas Utilizadas
Aqui est√£o as ferramentas que usamos para construir o projeto:
- **Python 3.x**: Linguagem principal, perfeita para experimentos em ML.
- **Pygame**: Cria a interface gr√°fica do labirinto, mostrando o rob√¥, tesouro e obst√°culos.
- **NumPy**: Gerencia a Q-Table, que armazena o aprendizado do rob√¥.
- **Matplotlib**: Gera gr√°ficos para visualizar o progresso do treinamento.
- **Random**: Permite a√ß√µes aleat√≥rias durante a explora√ß√£o inicial.
- **OS e Datetime**: Salva arquivos como logs e a Q-Table.

Essas bibliotecas s√£o amplamente usadas em projetos de ML e s√£o f√°ceis de instalar.

---

## üß† Algoritmos Aplicados
O projeto utiliza dois algoritmos principais para ensinar o rob√¥:
1. **Q-Learning**:
   - Um m√©todo de aprendizado por refor√ßo que ajuda o rob√¥ a aprender as melhores a√ß√µes (movimentos) com base em recompensas.
   - Atualiza uma Q-Table dinamicamente, funcionando como uma "mem√≥ria" do rob√¥ sobre o que funciona ou n√£o.
2. **Epsilon-Greedy**:
   - Balanceia **explora√ß√£o** (tentar a√ß√µes novas) e **explora√ß√£o** (usar o que j√° foi aprendido).
   - No in√≠cio, o rob√¥ explora mais (a√ß√µes aleat√≥rias); com o tempo, ele confia mais na Q-Table.

Esses algoritmos permitem que o rob√¥ reduza incertezas e encontre um caminho eficiente.

---

## üìä C√°lculos e F√≥rmulas Utilizados
O aprendizado do rob√¥ √© baseado em c√°lculos matem√°ticos simples. Aqui est√£o as f√≥rmulas principais:
1. **Atualiza√ß√£o da Q-Table**:
   \[
   Q(s, a) \leftarrow Q(s, a) + \alpha \cdot \left( r + \gamma \cdot \max Q(s', a') - Q(s, a) \right)
   \]
   - \( s \): Posi√ß√£o atual (ex.: (0,0)).
   - \( a \): A√ß√£o (cima, baixo, esquerda, direita).
   - \( s' \): Pr√≥xima posi√ß√£o.
   - \( r \): Recompensa (+20 por tesouro, -10 por obst√°culos, -0.2 por passo).
   - \( \alpha = 0.1 \): Taxa de aprendizado.
   - \( \gamma = 0.95 \): Fator de desconto (valoriza recompensas futuras).

2. **Ajuste da Taxa de Explora√ß√£o**:
   \[
   \epsilon \leftarrow \max(0.05, \epsilon \cdot 0.99)
   \]
   - \( \epsilon \): Taxa de explora√ß√£o, come√ßa em 1.0 (100% de a√ß√µes aleat√≥rias) e decai at√© 0.05 (5%), incentivando o rob√¥ a usar o aprendizado.

Essas f√≥rmulas guiam o rob√¥ para melhorar suas decis√µes a cada tentativa.

---

## üöÄ Como Executar o Projeto
Siga estes passos para rodar o projeto e ver o rob√¥ em a√ß√£o:

1. **Pr√©-requisitos**:
   - Instale o Python 3.x: [python.org](https://www.python.org/downloads/).
   - Instale as bibliotecas necess√°rias:
     ```bash
     pip install pygame numpy matplotlib
Baixe o C√≥digo:
Fa√ßa o download do arquivo explorador_labirinto_2d.py deste reposit√≥rio.
Execute o Programa:
Abra o terminal e navegue at√© a pasta do projeto:
powershell

Copiar
cd caminho/para/RotaPerfeita
Rode o c√≥digo:
powershell

Copiar
python explorador_labirinto_2d.py
Interaja com o Labirinto:
Durante o treinamento (1500 epis√≥dios), o Pygame mostra o rob√¥ a cada 100 epis√≥dios.
No modo interativo (ap√≥s o treinamento):
Espa√ßo: Pausa/retoma a anima√ß√£o.
r: Reinicia o rob√¥ na posi√ß√£o inicial (0,0).
Feche a janela para encerrar.
Gr√°ficos ser√£o exibidos ao final. Salve-os como graficos_treinamento.png (clique no √≠cone de disquete no Matplotlib).
Verifique os Resultados:
O caminho final aparece no console, ex.: [(0,0), (1,0), (2,0), (3,0), (4,0), (4,1), (4,2), (4,3), (4,4)].
Veja o arquivo log_treino.txt para m√©tricas e q_table.npy para a Q-Table salva.
√â simples e interativo, perfeito para aprender na pr√°tica!

üìà Resultados e Coment√°rios Finais
Resultados Obtidos
Ap√≥s 1500 epis√≥dios de treinamento, o rob√¥ aprendeu a navegar o labirinto com efici√™ncia:

Caminho Final:
text

Copiar
[(0,0), (1,0), (2,0), (3,0), (4,0), (4,1), (4,2), (4,3), (4,4)]
O rob√¥ alcan√ßa o tesouro em 9 passos, evitando obst√°culos e paredes.
Recompensa: Come√ßa negativa (ex.: -46.40 no epis√≥dio 0) e estabiliza entre +18 e +20, refletindo o sucesso (+20 por tesouro, menos -0.2 por passo).
Passos: Reduz de ~140 para ~9, mostrando efici√™ncia.
Explora√ß√£o (( \epsilon )): Cai de 1.0 para 0.05, indicando que o rob√¥ passou a confiar no aprendizado.
Visualiza√ß√£o do Labirinto
Epis√≥dio 0 (In√≠cio do Treinamento):
O rob√¥ (c√≠rculo azul) est√° em (0,0), tesouro (dourado) em (4,4), obst√°culos (vermelhos) em (1,1), (2,3), (3,2), e paredes (cinza).
Recompensa: -46.40, Passos: 36. Mostra a explora√ß√£o inicial, com penalidades por obst√°culos (-10) e passos (-0.2 cada).
Gr√°ficos de Treinamento
Evolu√ß√£o do Aprendizado:
Recompensas (azul): Sobem de -400 (explora√ß√£o inicial) para ~0, indicando aprendizado.
Passos (laranja): Caem de 140 para ~9, mostrando efici√™ncia.
Epsilon (verde): Decai de 1.0 para 0.05, reduzindo a√ß√µes aleat√≥rias.
Coment√°rios Finais
Este projeto demonstra os fundamentos do aprendizado por refor√ßo de forma clara e visual:

Conceitos Aplicados:
Redu√ß√£o de incertezas (explora√ß√£o vs. explora√ß√£o com epsilon-greedy).
Atualiza√ß√£o din√¢mica (Q-Table ajustada a cada a√ß√£o).
Recompensas e penalidades (+20 por tesouro, -10 por obst√°culos, -0.2 por passo).
Visualiza√ß√£o do comportamento aprendido (labirinto e gr√°ficos).
Dificuldades:
Ajustar o labirinto de 6x6 para 5x5 devido a erros na Q-Table.
Fallback para c√≠rculos, j√° que sprites n√£o foram fornecidos.
Sugest√µes de Melhorias:
Adicionar sprites para um visual mais rico.
Incluir sons (ex.: som de vit√≥ria ao alcan√ßar o tesouro).
Criar n√≠veis mais complexos ou obst√°culos din√¢micos.
O Explorador do Labirinto 2D √© uma √≥tima introdu√ß√£o ao Q-Learning e aprendizado por refor√ßo. Se voc√™ est√° come√ßando em ML, este projeto √© um ponto de partida pr√°tico e divertido. Explore, modifique e aprenda! üöÄ

‚ÑπÔ∏è Sobre a Pasta do Projeto
A pasta RotaPerfeita cont√©m:

explorador_labirinto_2d.py: C√≥digo-fonte completo.
README.md: Este arquivo com documenta√ß√£o.
q_table.npy: Q-Table salva.
log_treino.txt: Log do treinamento.
Imagens: labirinto_inicio.png e graficos_treinamento.png.
Se tiver d√∫vidas ou sugest√µes, sinta-se √† vontade para contribuir no reposit√≥rio!